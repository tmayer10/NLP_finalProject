{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Averaging Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a DAN to solve our problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot --quiet\n",
    "!pip install gensim --quiet\n",
    "!pip install tensorflow-datasets --quiet\n",
    "!pip install tensorflow-text --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"../dataset/tagged_transcripts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1962-houston_oilers-dallas_texans.txt</th>\n",
       "      <th>1969-chicago_bears-green_bay_packers.txt</th>\n",
       "      <th>1969-cleveland_browns-minnesota_vikings-1.txt</th>\n",
       "      <th>1969-cleveland_browns-minnesota_vikings.txt</th>\n",
       "      <th>1969-new_york_jets-baltimore_colts.txt</th>\n",
       "      <th>1970-baltimore_colts-kansas_city_chiefs.txt</th>\n",
       "      <th>1970-cleveland_browns-new_york_jets.txt</th>\n",
       "      <th>1970-dallas_cowboys-detroit_lions.txt</th>\n",
       "      <th>1970-kansas_city_chiefs-baltimore_colts.txt</th>\n",
       "      <th>1970-los_angeles_rams-minnesota_vikings-1.txt</th>\n",
       "      <th>...</th>\n",
       "      <th>2018-tampa_bay_buccaneers-dallas_cowboys.txt</th>\n",
       "      <th>2018-tampa_bay_buccaneers-detroit_lions.txt</th>\n",
       "      <th>2018-tennessee_titans-green_bay_packers.txt</th>\n",
       "      <th>2018-tennessee_titans-minnesota_vikings.txt</th>\n",
       "      <th>2018-tennessee_titans-pittsburgh_steelers.txt</th>\n",
       "      <th>2018-tennessee_titans-tampa_bay_buccaneers.txt</th>\n",
       "      <th>2018-washington_redskins-new_england_patriots.txt</th>\n",
       "      <th>2018-washington_redskins-new_york_jets.txt</th>\n",
       "      <th>2018-washington_redskins-philadelphia_eagles-1.txt</th>\n",
       "      <th>2018-washington_redskins-philadelphia_eagles.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>teams</th>\n",
       "      <td>[houston_oilers, dallas_texans]</td>\n",
       "      <td>[chicago_bears, green_bay_packers]</td>\n",
       "      <td>[cleveland_browns, minnesota_vikings]</td>\n",
       "      <td>[cleveland_browns, minnesota_vikings]</td>\n",
       "      <td>[new_york_jets, baltimore_colts]</td>\n",
       "      <td>[baltimore_colts, kansas_city_chiefs]</td>\n",
       "      <td>[cleveland_browns, new_york_jets]</td>\n",
       "      <td>[dallas_cowboys, detroit_lions]</td>\n",
       "      <td>[kansas_city_chiefs, baltimore_colts]</td>\n",
       "      <td>[los_angeles_rams, minnesota_vikings]</td>\n",
       "      <td>...</td>\n",
       "      <td>[tampa_bay_buccaneers, dallas_cowboys]</td>\n",
       "      <td>[tampa_bay_buccaneers, detroit_lions]</td>\n",
       "      <td>[tennessee_titans, green_bay_packers]</td>\n",
       "      <td>[tennessee_titans, minnesota_vikings]</td>\n",
       "      <td>[tennessee_titans, pittsburgh_steelers]</td>\n",
       "      <td>[tennessee_titans, tampa_bay_buccaneers]</td>\n",
       "      <td>[washington_redskins, new_england_patriots]</td>\n",
       "      <td>[washington_redskins, new_york_jets]</td>\n",
       "      <td>[washington_redskins, philadelphia_eagles]</td>\n",
       "      <td>[washington_redskins, philadelphia_eagles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transcript</th>\n",
       "      <td>gilson well defend the goal on your left theyl...</td>\n",
       "      <td>cbs television sports presents the national fo...</td>\n",
       "      <td>the nfl today brought to you by the foundation...</td>\n",
       "      <td>the nfl today brought to you by the foundation...</td>\n",
       "      <td>&amp;gt;&amp;gt; nbc sports presents the third nflafl ...</td>\n",
       "      <td>biochemistry was almost an that i doing it cam...</td>\n",
       "      <td>from municipal stadium in cleveland ohio to po...</td>\n",
       "      <td>a long time ago ford motor company had a bette...</td>\n",
       "      <td>from memorial stadium in baltimore maryland na...</td>\n",
       "      <td>from metropolitan stadium in bloomington minne...</td>\n",
       "      <td>...</td>\n",
       "      <td>you welcomes you to the following presentation...</td>\n",
       "      <td>well the rain continues to fall but we have fo...</td>\n",
       "      <td>so the first preseason game a couple weeks at ...</td>\n",
       "      <td>time is running out for some opportunity has c...</td>\n",
       "      <td>heinz field and the new head coach of the tita...</td>\n",
       "      <td>tennessee titans preseason football is brought...</td>\n",
       "      <td>the patriots take the field and football retur...</td>\n",
       "      <td>espn welcomes you to the following presentatio...</td>\n",
       "      <td>and there is nick falls the eagle fans at atte...</td>\n",
       "      <td>skins and eagles theyve been division rivals d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>1962</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 1455 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        1962-houston_oilers-dallas_texans.txt  \\\n",
       "teams                         [houston_oilers, dallas_texans]   \n",
       "transcript  gilson well defend the goal on your left theyl...   \n",
       "year                                                     1962   \n",
       "\n",
       "                     1969-chicago_bears-green_bay_packers.txt  \\\n",
       "teams                      [chicago_bears, green_bay_packers]   \n",
       "transcript  cbs television sports presents the national fo...   \n",
       "year                                                     1969   \n",
       "\n",
       "                1969-cleveland_browns-minnesota_vikings-1.txt  \\\n",
       "teams                   [cleveland_browns, minnesota_vikings]   \n",
       "transcript  the nfl today brought to you by the foundation...   \n",
       "year                                                     1969   \n",
       "\n",
       "                  1969-cleveland_browns-minnesota_vikings.txt  \\\n",
       "teams                   [cleveland_browns, minnesota_vikings]   \n",
       "transcript  the nfl today brought to you by the foundation...   \n",
       "year                                                     1969   \n",
       "\n",
       "                       1969-new_york_jets-baltimore_colts.txt  \\\n",
       "teams                        [new_york_jets, baltimore_colts]   \n",
       "transcript  &gt;&gt; nbc sports presents the third nflafl ...   \n",
       "year                                                     1969   \n",
       "\n",
       "                  1970-baltimore_colts-kansas_city_chiefs.txt  \\\n",
       "teams                   [baltimore_colts, kansas_city_chiefs]   \n",
       "transcript  biochemistry was almost an that i doing it cam...   \n",
       "year                                                     1970   \n",
       "\n",
       "                      1970-cleveland_browns-new_york_jets.txt  \\\n",
       "teams                       [cleveland_browns, new_york_jets]   \n",
       "transcript  from municipal stadium in cleveland ohio to po...   \n",
       "year                                                     1970   \n",
       "\n",
       "                        1970-dallas_cowboys-detroit_lions.txt  \\\n",
       "teams                         [dallas_cowboys, detroit_lions]   \n",
       "transcript  a long time ago ford motor company had a bette...   \n",
       "year                                                     1970   \n",
       "\n",
       "                  1970-kansas_city_chiefs-baltimore_colts.txt  \\\n",
       "teams                   [kansas_city_chiefs, baltimore_colts]   \n",
       "transcript  from memorial stadium in baltimore maryland na...   \n",
       "year                                                     1970   \n",
       "\n",
       "                1970-los_angeles_rams-minnesota_vikings-1.txt  ...  \\\n",
       "teams                   [los_angeles_rams, minnesota_vikings]  ...   \n",
       "transcript  from metropolitan stadium in bloomington minne...  ...   \n",
       "year                                                     1970  ...   \n",
       "\n",
       "                 2018-tampa_bay_buccaneers-dallas_cowboys.txt  \\\n",
       "teams                  [tampa_bay_buccaneers, dallas_cowboys]   \n",
       "transcript  you welcomes you to the following presentation...   \n",
       "year                                                     2018   \n",
       "\n",
       "                  2018-tampa_bay_buccaneers-detroit_lions.txt  \\\n",
       "teams                   [tampa_bay_buccaneers, detroit_lions]   \n",
       "transcript  well the rain continues to fall but we have fo...   \n",
       "year                                                     2018   \n",
       "\n",
       "                  2018-tennessee_titans-green_bay_packers.txt  \\\n",
       "teams                   [tennessee_titans, green_bay_packers]   \n",
       "transcript  so the first preseason game a couple weeks at ...   \n",
       "year                                                     2018   \n",
       "\n",
       "                  2018-tennessee_titans-minnesota_vikings.txt  \\\n",
       "teams                   [tennessee_titans, minnesota_vikings]   \n",
       "transcript  time is running out for some opportunity has c...   \n",
       "year                                                     2018   \n",
       "\n",
       "                2018-tennessee_titans-pittsburgh_steelers.txt  \\\n",
       "teams                 [tennessee_titans, pittsburgh_steelers]   \n",
       "transcript  heinz field and the new head coach of the tita...   \n",
       "year                                                     2018   \n",
       "\n",
       "               2018-tennessee_titans-tampa_bay_buccaneers.txt  \\\n",
       "teams                [tennessee_titans, tampa_bay_buccaneers]   \n",
       "transcript  tennessee titans preseason football is brought...   \n",
       "year                                                     2018   \n",
       "\n",
       "            2018-washington_redskins-new_england_patriots.txt  \\\n",
       "teams             [washington_redskins, new_england_patriots]   \n",
       "transcript  the patriots take the field and football retur...   \n",
       "year                                                     2018   \n",
       "\n",
       "                   2018-washington_redskins-new_york_jets.txt  \\\n",
       "teams                    [washington_redskins, new_york_jets]   \n",
       "transcript  espn welcomes you to the following presentatio...   \n",
       "year                                                     2018   \n",
       "\n",
       "           2018-washington_redskins-philadelphia_eagles-1.txt  \\\n",
       "teams              [washington_redskins, philadelphia_eagles]   \n",
       "transcript  and there is nick falls the eagle fans at atte...   \n",
       "year                                                     2018   \n",
       "\n",
       "             2018-washington_redskins-philadelphia_eagles.txt  \n",
       "teams              [washington_redskins, philadelphia_eagles]  \n",
       "transcript  skins and eagles theyve been division rivals d...  \n",
       "year                                                     2018  \n",
       "\n",
       "[3 rows x 1455 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/tommayer/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('word2vec_sample')\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "\n",
    "wvmodel = KeyedVectors.load_word2vec_format(datapath(word2vec_sample), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wvmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this dataset has 43,981 games over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text:\n",
    "- remove punctuation\n",
    "- replace with space \n",
    "- also lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transposed = data.T.reset_index().rename(columns={'index': 'game_id'}) # pd operation\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each transcript\n",
    "data_transposed['tokens'] = data_transposed['transcript'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>teams</th>\n",
       "      <th>transcript</th>\n",
       "      <th>year</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-houston_oilers-dallas_texans.txt</td>\n",
       "      <td>[houston_oilers, dallas_texans]</td>\n",
       "      <td>gilson well defend the goal on your left theyl...</td>\n",
       "      <td>1962</td>\n",
       "      <td>[gilson, well, defend, the, goal, on, your, le...</td>\n",
       "      <td>[0.02728455, 0.016727475, 0.0260244, 0.0380681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1969-chicago_bears-green_bay_packers.txt</td>\n",
       "      <td>[chicago_bears, green_bay_packers]</td>\n",
       "      <td>cbs television sports presents the national fo...</td>\n",
       "      <td>1969</td>\n",
       "      <td>[cbs, television, sports, presents, the, natio...</td>\n",
       "      <td>[0.030220592, 0.014963325, 0.02284711, 0.03831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1969-cleveland_browns-minnesota_vikings-1.txt</td>\n",
       "      <td>[cleveland_browns, minnesota_vikings]</td>\n",
       "      <td>the nfl today brought to you by the foundation...</td>\n",
       "      <td>1969</td>\n",
       "      <td>[the, nfl, today, brought, to, you, by, the, f...</td>\n",
       "      <td>[0.027876755, 0.016259313, 0.022658505, 0.0399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1969-cleveland_browns-minnesota_vikings.txt</td>\n",
       "      <td>[cleveland_browns, minnesota_vikings]</td>\n",
       "      <td>the nfl today brought to you by the foundation...</td>\n",
       "      <td>1969</td>\n",
       "      <td>[the, nfl, today, brought, to, you, by, the, f...</td>\n",
       "      <td>[0.028167814, 0.016339412, 0.022509856, 0.0396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1969-new_york_jets-baltimore_colts.txt</td>\n",
       "      <td>[new_york_jets, baltimore_colts]</td>\n",
       "      <td>&amp;gt;&amp;gt; nbc sports presents the third nflafl ...</td>\n",
       "      <td>1969</td>\n",
       "      <td>[gtgt, nbc, sports, presents, the, third, nfla...</td>\n",
       "      <td>[0.031091398, 0.015320361, 0.02407883, 0.03909...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         game_id  \\\n",
       "0          1962-houston_oilers-dallas_texans.txt   \n",
       "1       1969-chicago_bears-green_bay_packers.txt   \n",
       "2  1969-cleveland_browns-minnesota_vikings-1.txt   \n",
       "3    1969-cleveland_browns-minnesota_vikings.txt   \n",
       "4         1969-new_york_jets-baltimore_colts.txt   \n",
       "\n",
       "                                   teams  \\\n",
       "0        [houston_oilers, dallas_texans]   \n",
       "1     [chicago_bears, green_bay_packers]   \n",
       "2  [cleveland_browns, minnesota_vikings]   \n",
       "3  [cleveland_browns, minnesota_vikings]   \n",
       "4       [new_york_jets, baltimore_colts]   \n",
       "\n",
       "                                          transcript  year  \\\n",
       "0  gilson well defend the goal on your left theyl...  1962   \n",
       "1  cbs television sports presents the national fo...  1969   \n",
       "2  the nfl today brought to you by the foundation...  1969   \n",
       "3  the nfl today brought to you by the foundation...  1969   \n",
       "4  &gt;&gt; nbc sports presents the third nflafl ...  1969   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [gilson, well, defend, the, goal, on, your, le...   \n",
       "1  [cbs, television, sports, presents, the, natio...   \n",
       "2  [the, nfl, today, brought, to, you, by, the, f...   \n",
       "3  [the, nfl, today, brought, to, you, by, the, f...   \n",
       "4  [gtgt, nbc, sports, presents, the, third, nfla...   \n",
       "\n",
       "                                       doc_embedding  \n",
       "0  [0.02728455, 0.016727475, 0.0260244, 0.0380681...  \n",
       "1  [0.030220592, 0.014963325, 0.02284711, 0.03831...  \n",
       "2  [0.027876755, 0.016259313, 0.022658505, 0.0399...  \n",
       "3  [0.028167814, 0.016339412, 0.022509856, 0.0396...  \n",
       "4  [0.031091398, 0.015320361, 0.02407883, 0.03909...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transposed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a column of tokens that we can use to get the game commentary embeddings.  We also have each game as a different row making it easier to work with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the game commentary embeddings. Essentially, computers can do math much more easily with numbers than with text.  So, we'll convert the text into numbers saving compute with a pretrained model (word2vec that I called wvmodel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tokens, model):\n",
    "    # Filter tokens to only those in the model's vocabulary\n",
    "    valid_tokens = [token for token in tokens if token in model.key_to_index]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Average the word vectors\n",
    "    return np.mean([model[token] for token in valid_tokens], axis=0)\n",
    "\n",
    "# Apply to each game transcript\n",
    "data_transposed['doc_embedding'] = data_transposed['tokens'].apply(\n",
    "    lambda tokens: get_document_embedding(tokens, wvmodel)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the embedding matrix.  This converts the w2v model, that we are using already, into a matrix that we can use for our model. Then, we build a vocabulary dictionary that we can use to map the words to their corresponding indices.  We cannot forget to add the unknown token to the vocabulary dictionary too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = len(wvmodel['university'])      # we know... it's 300\n",
    "\n",
    "# initialize embedding matrix and word-to-id map:\n",
    "embedding_matrix = np.zeros((len(wvmodel) + 1, EMBEDDING_DIM))\n",
    "vocab_dict = {}\n",
    "\n",
    "# build the embedding matrix and the word-to-id map:\n",
    "for i, word in enumerate(wvmodel.index_to_key):\n",
    "    embedding_vector = wvmodel[word]\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        vocab_dict[word] = i\n",
    "\n",
    "# we can use the last index at the end of the vocab for unknown tokens\n",
    "vocab_dict['[UNK]'] = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43982, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek at the embedding matrix\n",
    "embedding_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0891758 ,  0.121832  , -0.0671959 ,  0.0477279 , -0.013659  ,\n",
       "       -0.0671959 ,  0.0640559 , -0.0331269 , -0.0364239 ,  0.00565199,\n",
       "       -0.017113  , -0.10362   ,  0.0552639 , -0.00706499, -0.0643699 ,\n",
       "        0.00753598, -0.0866638 ,  0.0492979 , -0.0816398 , -0.0910598 ,\n",
       "        0.00416049, -0.0681379 ,  0.0568339 ,  0.0524379 ,  0.00143262,\n",
       "       -0.01256   , -0.0775578 ,  0.0960838 ,  0.0555779 , -0.0734758 ,\n",
       "       -0.013659  , -0.0376799 , -0.0489839 , -0.0470999 , -0.102992  ,\n",
       "        0.00612299,  0.0452159 , -0.0356389 ,  0.0665679 ,  0.0747318 ,\n",
       "        0.0759878 , -0.0248059 ,  0.013031  , -0.00490624,  0.00733973,\n",
       "       -0.0351679 ,  0.00639774, -0.00370912,  0.0835238 ,  0.0477279 ,\n",
       "       -0.0885478 , -0.0929438 ,  0.0634279 ,  0.0741038 ,  0.00561274,\n",
       "       -0.0192325 ,  0.0803838 ,  0.00580899,  0.0923158 ,  0.0700219 ,\n",
       "        0.0266899 ,  0.0788138 , -0.0634279 , -0.0470999 ,  0.0835238 ,\n",
       "       -0.0483559 ,  0.0574619 ,  0.0411339 ,  0.00455299,  0.0712778 ,\n",
       "        0.0769298 , -0.0173485 , -0.10676   , -0.0343829 ,  0.00134431,\n",
       "        0.0753598 , -0.00470999, -0.0584039 ,  0.0967118 ,  0.0467859 ,\n",
       "       -0.010519  , -0.0279459 ,  0.0265329 , -0.013659  , -0.14444   ,\n",
       "        0.0430179 ,  0.0778718 , -0.0558919 , -0.0285739 , -0.021352  ,\n",
       "       -0.0430179 ,  0.0665679 , -0.0508679 ,  0.0543219 , -0.0323419 ,\n",
       "       -0.1099    ,  0.0365809 , -0.00549499, -0.012403  , -0.158884  ,\n",
       "        0.0400349 , -0.11618   ,  0.105504  , -0.0898038 ,  0.010676  ,\n",
       "       -0.0477279 , -0.0345399 , -0.0324989 ,  0.00347362, -0.0241779 ,\n",
       "       -0.0458439 ,  0.0323419 ,  0.0659399 ,  0.0693939 ,  0.0659399 ,\n",
       "       -0.0361099 ,  0.0346969 , -0.114296  , -0.118064  ,  0.0268469 ,\n",
       "       -0.0470999 ,  0.020253  , -0.013188  , -0.0383079 , -0.023864  ,\n",
       "        0.0719059 ,  0.0697079 ,  0.0847798 ,  0.0910598 , -0.0244919 ,\n",
       "       -0.00859573,  0.0365809 ,  0.013345  ,  0.0756738 , -0.0414479 ,\n",
       "        0.00506324, -0.0102835 ,  0.0430179 ,  0.0904318 ,  0.0653119 ,\n",
       "        0.0602879 , -0.0562059 , -0.0246489 ,  0.020567  ,  0.0784998 ,\n",
       "        0.0954558 , -0.0274749 ,  0.0753598 ,  0.00938073, -0.0973398 ,\n",
       "        0.0788138 ,  0.019311  , -0.016485  ,  0.0496119 , -0.012089  ,\n",
       "       -0.00135412, -0.0394069 ,  0.0593459 , -0.0176625 ,  0.0521239 ,\n",
       "       -0.0731618 ,  0.0255909 ,  0.020253  , -0.0401919 , -0.00266899,\n",
       "       -0.0728479 , -0.0290449 ,  0.00304187,  0.00107447,  0.0518099 ,\n",
       "       -0.0120105 , -0.0392499 , -0.022765  ,  0.00651549, -0.0427039 ,\n",
       "       -0.0449019 ,  0.0649979 ,  0.0102835 , -0.0967118 , -0.0390929 ,\n",
       "       -0.148836  , -0.118692  ,  0.0602879 ,  0.0126385 , -0.0464719 ,\n",
       "        0.0271609 , -0.0778718 , -0.0587179 , -0.013816  ,  0.0524379 ,\n",
       "        0.0417619 , -0.133764  ,  0.0373659 , -0.01413   ,  0.101736  ,\n",
       "        0.111784  ,  0.021195  ,  0.0433319 ,  0.0240209 ,  0.101736  ,\n",
       "       -0.0527519 , -0.021823  , -0.0284169 , -0.0276319 , -0.00098615,\n",
       "       -0.016014  , -0.0149935 ,  0.0998518 ,  0.0712778 , -0.0470999 ,\n",
       "       -0.0279459 , -0.0387789 , -0.00170737, -0.0609159 , -0.0979678 ,\n",
       "        0.0483559 ,  0.0340689 , -0.138788  ,  0.0439599 , -0.0342259 ,\n",
       "        0.011775  ,  0.00761448,  0.00922373, -0.0173485 ,  0.0458439 ,\n",
       "       -0.0584039 ,  0.00608374, -0.00522024,  0.0178195 , -0.016171  ,\n",
       "        0.00604449, -0.0898038 , -0.020567  ,  0.013973  ,  0.0527519 ,\n",
       "       -0.016014  ,  0.0127955 , -0.0271609 ,  0.0401919 ,  0.0397209 ,\n",
       "       -0.00710424, -0.0656259 ,  0.01256   , -0.0847798 , -0.0442739 ,\n",
       "        0.0339119 ,  0.0401919 ,  0.014915  , -0.0637419 ,  0.0290449 ,\n",
       "       -0.0179765 ,  0.0251199 , -0.0615439 ,  0.0499259 ,  0.0164065 ,\n",
       "        0.0386219 , -0.0609159 ,  0.0800698 , -0.0822678 ,  0.0310859 ,\n",
       "       -0.0788138 , -0.00663324,  0.0127955 ,  0.020567  ,  0.0835238 ,\n",
       "        0.110528  , -0.108644  ,  0.00374837, -0.020567  , -0.0464719 ,\n",
       "       -0.015386  ,  0.0979678 , -0.023864  , -0.012717  ,  0.0251199 ,\n",
       "       -0.0389359 ,  0.0828958 ,  0.10676   ,  0.0390929 ,  0.0756738 ,\n",
       "        0.0140515 , -0.021823  ,  0.16202401,  0.0941998 , -0.0118535 ,\n",
       "       -0.0452159 , -0.0298299 ,  0.0423899 ,  0.0712778 ,  0.00241387,\n",
       "       -0.00883123,  0.0577759 , -0.0189185 ,  0.0168775 ,  0.0408199 ,\n",
       "       -0.0405059 ,  0.0552639 , -0.0480419 , -0.0277889 ,  0.0872918 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and take a look at the first embedding vector, a game from 1962!\n",
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to group the data into 'eras' to make it easier to predict. I will group the data by decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things here:\n",
    "- I'll use indices to split the data. This way, I can keep the columns together aka maintain the relationships between the columns.\n",
    "- I'll stratify by year to ensure temporal representation across both sets.\n",
    "- I'll use 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the year is an int\n",
    "data_transposed['year'] = data_transposed['year'].astype(int)\n",
    "# now group by decade\n",
    "data_transposed['decade'] = (data_transposed['year'] // 10) * 10\n",
    "indices_train, indices_test = train_test_split(\n",
    "    np.arange(len(data_transposed)),\n",
    "    test_size=0.2,\n",
    "    stratify=data_transposed['decade']  # Stratify by decade instead\n",
    ")\n",
    "\n",
    "# Create train and test DataFrames\n",
    "train_df = data_transposed.iloc[indices_train].copy()\n",
    "test_df = data_transposed.iloc[indices_test].copy()\n",
    "\n",
    "# Extract features and targets\n",
    "X_train = np.array(train_df['doc_embedding'].tolist())\n",
    "X_test = np.array(test_df['doc_embedding'].tolist())\n",
    "y_train = train_df['decade'].values  \n",
    "y_test = test_df['decade'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.80200057e-02,  1.85463410e-02,  2.20466256e-02,  4.07019816e-02,\n",
       "       -1.83194838e-02, -2.74180733e-02,  1.18565168e-02, -4.72977795e-02,\n",
       "        3.47229578e-02,  3.66538689e-02, -1.77515280e-02, -5.40170372e-02,\n",
       "       -1.39009790e-03,  7.09863938e-03, -5.18067777e-02,  1.09228622e-02,\n",
       "        2.75296438e-02,  3.47514078e-02,  6.25449792e-03, -2.42791437e-02,\n",
       "       -1.37908487e-02,  2.85209920e-02, -1.74424052e-03, -9.51399282e-03,\n",
       "        3.26409712e-02, -2.68419436e-03, -3.48200053e-02,  2.01653540e-02,\n",
       "        1.49998190e-02,  1.11482479e-02, -1.02505460e-02,  1.28404219e-02,\n",
       "       -1.61986034e-02, -1.00000612e-02,  1.00300312e-02,  3.64408130e-03,\n",
       "       -8.78093822e-04, -1.46641312e-02,  1.89352762e-02,  4.04345915e-02,\n",
       "        4.28580418e-02, -3.64055224e-02,  5.01380898e-02, -1.28889643e-02,\n",
       "       -6.42580539e-03,  4.69247764e-03, -1.17599135e-02,  3.78651032e-03,\n",
       "        2.27206238e-02,  1.10230464e-02,  1.21068703e-02,  2.77537610e-02,\n",
       "       -5.08523453e-03, -1.26083205e-02, -9.33056604e-03,  1.71077624e-02,\n",
       "       -3.55955539e-03, -1.52762989e-02,  2.10290477e-02, -3.06579396e-02,\n",
       "       -2.44658589e-02,  3.85987498e-02, -3.79147567e-02, -4.79277894e-02,\n",
       "       -6.96453964e-03, -1.34304026e-03, -3.63877811e-03,  4.15009372e-02,\n",
       "       -7.57232914e-03,  4.94802482e-02,  2.54540369e-02,  5.34386653e-03,\n",
       "        3.32267098e-02, -5.63935668e-04, -5.19457720e-02, -3.07706594e-02,\n",
       "        4.49639969e-02,  4.86782603e-02,  1.19826123e-02,  4.96647470e-02,\n",
       "        1.46868872e-02, -4.36595716e-02,  2.76336391e-02, -4.10324847e-03,\n",
       "       -7.08883256e-03, -3.50742936e-02, -3.57852615e-02,  4.92719151e-02,\n",
       "       -7.77348550e-03,  5.14199911e-03,  1.95777547e-02,  3.12781595e-02,\n",
       "       -2.53570452e-02, -4.84025925e-02, -2.06676815e-02, -3.42861041e-02,\n",
       "        1.91312246e-02,  1.94648840e-02, -7.35572353e-03,  7.77842104e-03,\n",
       "       -1.76453739e-02, -3.23282294e-02,  1.27490088e-02,  5.42868441e-03,\n",
       "       -2.46183928e-02, -1.94446109e-02, -1.60452388e-02, -2.64864583e-02,\n",
       "        9.82564408e-03, -3.50787602e-02, -1.25218043e-02, -3.01643944e-04,\n",
       "       -2.06577741e-02, -6.64575910e-03,  2.72439010e-02,  1.09517118e-02,\n",
       "        2.79362053e-02, -2.95597070e-04,  3.49609293e-02,  2.29729470e-02,\n",
       "       -5.65465018e-02, -6.51026145e-03, -3.08735278e-02,  3.55066732e-02,\n",
       "       -2.82495711e-02, -1.90710574e-02, -1.71186309e-02, -3.24942209e-02,\n",
       "        2.37680716e-03,  1.39574679e-02, -1.71917565e-02, -5.06116040e-02,\n",
       "       -4.32492234e-02, -2.18834635e-02, -1.55092198e-02, -3.91912386e-02,\n",
       "        1.32318130e-02,  9.08087660e-03, -1.84911210e-02,  3.25760581e-02,\n",
       "        1.41889835e-02, -2.63573695e-02,  2.44173184e-02, -5.23859449e-03,\n",
       "        7.00813346e-03,  1.92133933e-02, -2.23753303e-02, -4.49449494e-02,\n",
       "       -3.24436724e-02,  9.47650708e-03,  2.29411274e-02,  2.85285357e-02,\n",
       "       -5.30348532e-02,  3.06378547e-02, -1.26673039e-02,  1.11612640e-02,\n",
       "       -2.05999110e-02, -4.73278873e-02, -1.13352872e-02, -1.90965129e-05,\n",
       "       -1.20246215e-02,  3.14698629e-02,  2.12595798e-02,  8.94986186e-03,\n",
       "        3.75326979e-03, -3.51622328e-02,  2.31292825e-02, -1.61139444e-02,\n",
       "        1.81736797e-02, -2.66742036e-02, -6.03820458e-02, -2.18530502e-02,\n",
       "       -1.33665781e-02, -4.63983528e-02, -2.04785671e-02, -1.45843597e-02,\n",
       "        3.13612968e-02, -3.23841758e-02, -4.33969358e-03,  2.04629870e-03,\n",
       "       -2.61238758e-02, -1.76512562e-02,  1.17939524e-02,  1.67378895e-02,\n",
       "       -1.44498739e-02, -1.08354790e-02, -1.98855996e-03,  1.31504098e-02,\n",
       "        5.44647351e-02,  1.24448128e-02,  2.36977190e-02,  1.28715578e-02,\n",
       "        2.16644425e-02,  5.97991189e-03, -2.78512258e-02,  3.95832863e-03,\n",
       "       -1.01834144e-02,  1.61442114e-03, -3.24887373e-02, -5.60123995e-02,\n",
       "        1.12671992e-02,  1.90234222e-02, -9.47519112e-03,  5.73936338e-03,\n",
       "        1.97764821e-02, -6.52481336e-03, -1.56867169e-02, -5.84428851e-03,\n",
       "        2.36384291e-02, -2.55137216e-03, -5.66018093e-03,  2.06442140e-02,\n",
       "       -1.39364144e-02,  2.25128848e-02, -4.91532758e-02,  7.38917431e-03,\n",
       "        4.31686267e-02, -6.97594322e-03, -5.44571951e-02,  1.82941571e-04,\n",
       "        5.09490725e-04, -2.63717491e-03, -5.00046462e-03, -2.31857002e-02,\n",
       "        3.83669734e-02, -2.04019900e-02,  2.83096433e-02,  2.84192674e-02,\n",
       "        1.34404395e-02,  2.90802750e-03,  1.06750485e-02, -2.75787897e-02,\n",
       "        1.76512375e-02,  1.15129575e-02,  1.58365611e-02, -8.38871859e-03,\n",
       "       -7.80001003e-03, -2.67534852e-02,  2.93802023e-02,  5.05815865e-03,\n",
       "        1.18189082e-02,  4.77763684e-03,  6.29902771e-03, -5.76729402e-02,\n",
       "        1.25831959e-03, -1.66815158e-03,  1.49027631e-02,  1.53953834e-02,\n",
       "        4.65612719e-03, -3.42844869e-03,  4.76622488e-03,  2.96381116e-03,\n",
       "        2.56236959e-02,  1.64394006e-02,  2.72486117e-02, -1.70552842e-02,\n",
       "        2.56514437e-02,  6.78198505e-03, -3.21477801e-02, -2.58731712e-02,\n",
       "       -7.78797257e-04,  4.59528761e-03, -2.94580460e-02,  2.23007277e-02,\n",
       "        3.01093664e-02,  5.90068586e-02,  4.38480545e-03,  6.01835176e-03,\n",
       "       -2.83216182e-02,  7.71057699e-03,  2.85049435e-02,  4.03921492e-02,\n",
       "        4.69466373e-02,  1.66306254e-02,  2.51757428e-02, -2.67669614e-02,\n",
       "       -2.29431707e-02, -6.79624453e-02, -1.74352489e-02, -1.63118038e-02,\n",
       "        1.27454270e-02, -1.10028805e-02,  8.09071213e-03,  3.68060991e-02,\n",
       "        7.52479117e-03, -1.03692361e-03, -3.70334387e-02, -1.35506028e-02,\n",
       "        7.44358683e-03,  8.57970398e-03, -2.94777267e-02,  1.70030650e-02,\n",
       "       -4.69154902e-02,  9.21216048e-03, -1.17300265e-02,  1.15118930e-02,\n",
       "        5.01748128e-03, -2.15475354e-02,  1.20003186e-02, -2.73311697e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Averaging Network\n",
    "Simple yet powerful baseline model.\n",
    "The DAN takes the average of the word embeddings to get a document embedding. In my model, it was average the embeddings of the game commentary to get a result of the decade.  It can show me semantic meaning of the game commentary learning complex patterns via a nueral network.  The hidden layers can serve as better feature representations than the raw averages of the vectors. As a baseline model, this DAN will give me more interpreatble results versus a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
